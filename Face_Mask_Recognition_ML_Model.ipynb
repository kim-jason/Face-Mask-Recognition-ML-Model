{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Face Mask Recognition ML Model",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjQm4zFAIYVG"
      },
      "source": [
        "#Mask Detection\n",
        "### In this section, you will implement the mask detector using different methods.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Rh0tlKBPb-C"
      },
      "source": [
        "## Download the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yEswIcfWvay"
      },
      "source": [
        "Train data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZTeb2zHQpgY"
      },
      "source": [
        "!wget \"https://drive.google.com/uc?export=download&id=1ouMOxaDNNr-U7E2DD3Y6wLlyeVb8qnr0\" -O train_data.p\n",
        "data_list = pickle.load(open(\"train_data.p\", \"rb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKVH6DRKWxeO"
      },
      "source": [
        "Test images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x__kgFbQWur2"
      },
      "source": [
        "!wget \"https://drive.google.com/uc?export=download&id=16CCS6DiAzFwCT1165ogvKVf9JwajOAHm\" -O test_images.p\n",
        "test_images = pickle.load(open(\"test_images.p\", \"rb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "off-m630cvum"
      },
      "source": [
        "### Get a View of how dataset looks like."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_eug-JMTPyV"
      },
      "source": [
        "Run the following code to check the dataset.\n",
        "\n",
        "The given training dataset contains 4602 examples and example is a human face with a label (good or bad) to denote whether there is a mask on the face."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkLVp8-hRixm"
      },
      "source": [
        "print(\"The size of the dataset is: \", len(data_list))\n",
        "print(\"\\n The Structure of the data: \", '\\n', type(data_list[0]))\n",
        "print(data_list[-1])\n",
        "print(\"\\n Show some of the samples\")\n",
        "f, axarr = plt.subplots(2,2, figsize=(10,10))\n",
        "axarr[0, 0].imshow(data_list[1]['image'])\n",
        "axarr[0, 0].title.set_text(data_list[0]['label'])\n",
        "axarr[0, 1].imshow(data_list[2]['image'])\n",
        "axarr[0, 1].title.set_text(data_list[2]['label'])\n",
        "axarr[1, 0].imshow(data_list[-1]['image'])\n",
        "axarr[1, 0].title.set_text(data_list[-1]['label'])\n",
        "axarr[1, 1].imshow(data_list[-2]['image'])\n",
        "axarr[1, 1].title.set_text(data_list[-3]['label'])\n",
        "f.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wy71-MqgZZ3q"
      },
      "source": [
        "## Preprocessing of the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4di7_f-QZjCQ"
      },
      "source": [
        "### Distribution of the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXGGL_AMTZnO"
      },
      "source": [
        "### Count the number of different classes ###\n",
        "Num_of_classes = 0\n",
        "classes = {}\n",
        "for sample in data_list:\n",
        "  if sample['label'] not in classes:\n",
        "    classes[sample['label']] = 1\n",
        "  else:\n",
        "    classes[sample['label']] += 1\n",
        "print(\"Num. of Different Classes: \", classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCbL3TcDdfBb"
      },
      "source": [
        "### Distribution of the Size of the Images ###\n",
        "image_size = []\n",
        "for sample in data_list:\n",
        "  image_data = sample['image']\n",
        "  size = image_data.shape[0] * image_data.shape[1]\n",
        "  image_size.append(size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6RynTMweX2z"
      },
      "source": [
        "print(\"Median Size of Images: \", np.median(image_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9bRDJBrhPp8"
      },
      "source": [
        "### Image Preprocessing\n",
        "For this section, we will preprocess the image data according to the following steps\n",
        "*   Convert gray image to RGB\n",
        "*   Resize the image to 128 * 128\n",
        "*   Scale the value of each pixel from [0, 255] to [-1, 1]\n",
        "\n",
        "You may find the opencv API is helpful."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlMUu2uShnA0"
      },
      "source": [
        "### Images & Labels Split ###\n",
        "label2int = {\"good\": 1, \"bad\": 0}\n",
        "IMAGES = []\n",
        "LABELS = []\n",
        "for sample in data_list:\n",
        "  IMAGES.append(sample['image'])\n",
        "  LABELS.append(label2int[sample['label']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFbpV_Q-h1tM"
      },
      "source": [
        "and from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "### Preprocess the Image ###\n",
        "def preprocess(image):\n",
        "  '''TODO\n",
        "  Preprocess the input image:\n",
        "  1. convert the gray-scale (2D) image to RGB (3D)\n",
        "  2. Resize the image to (128, 128, 3)\n",
        "  3. Scale the value of each pixel from [0, 255] to [-1, 1]\n",
        "  '''\n",
        "\n",
        "  #Converts the gray-scale (2D) image to RGB (3D)\n",
        "  image = cv.cvtColor(image.astype(np.uint8), cv.COLOR_GRAY2RGB)\n",
        "  \n",
        "  #Resizes the image to (128, 128, 3)\n",
        "  image = cv.resize(image, (128, 128))\n",
        "\n",
        "  #Scales the value of each pixel from [0, 255] to [-1, 1] using MinMaxScaler\n",
        "  scaler = MinMaxScaler(feature_range=(-1,1))\n",
        "  image = scaler.fit_transform(image.reshape(-1, image.shape[-1])).reshape(image.shape)\n",
        "\n",
        "  return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQuHlutBi_2X"
      },
      "source": [
        "### Process the Raw Image ###\n",
        "processed_IMAGES = []\n",
        "for image in IMAGES:\n",
        "  processed_IMAGES.append(preprocess(image))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ecnr-begcUl"
      },
      "source": [
        "#Tests with a sample image to see if pixel values have been properly scaled from [0,255] to [-1,1]\n",
        "test = processed_IMAGES[0]\n",
        "print(np.max(test)) #Should be 1\n",
        "print(np.min(test)) #Should be -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tf8i0zjcMElW"
      },
      "source": [
        "### Train / Validation Split\n",
        "Now we split the dataset to train and dev set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClQ9FDEBL71d"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(processed_IMAGES, LABELS, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0nbyr6d59iH"
      },
      "source": [
        "#https://towardsdatascience.com/deeppicar-part-5-lane-following-via-deep-learning-d93acdce6110\n",
        "from imgaug import augmenters\n",
        "import random\n",
        "\n",
        "class Augmenter:\n",
        "  def __init__(self, XData, YData):\n",
        "    self.XData = XData\n",
        "    self.YData = YData\n",
        "    self.run()\n",
        "\n",
        "  def zoom(self, data):\n",
        "    def yielder():\n",
        "      for img in data:\n",
        "          maxZoomLevel = random.uniform(1.1, 1.5)\n",
        "          zoom = augmenters.Affine(scale=(1, maxZoomLevel)) \n",
        "\n",
        "          yield zoom.augment_image(img) \n",
        "          \n",
        "    return np.array(list(yielder()))\n",
        "  def flip(self, data):\n",
        "    def yielder():\n",
        "      for img in data:\n",
        "          flipHorz = random.randint(0,1)\n",
        "          flipVert = random.randint(0,1)\n",
        "          if flipHorz and flipVert:\n",
        "            img = cv.flip(img,0)\n",
        "            yield cv.flip(img, 1)\n",
        "          elif flipHorz and not flipVert:\n",
        "            yield cv.flip(img, 0)\n",
        "          elif not flipHorz and flipVert:\n",
        "            yield cv.flip(img, 1)\n",
        "          else:\n",
        "            img = cv.flip(img,0)\n",
        "            yield cv.flip(img, 1)\n",
        "\n",
        "    return np.array(list(yielder()))\n",
        "  def blur(self, data):\n",
        "    def yielder():\n",
        "      for img in data:\n",
        "        blur = random.randint(0,1) \n",
        "        # if blur:\n",
        "        kernel_size = random.randint(3, 30)\n",
        "        yield cv.blur(img, (kernel_size,kernel_size))\n",
        "        # else:\n",
        "        #   yield img\n",
        "    return np.array(list(yielder()))\n",
        "\n",
        "  def run(self):\n",
        "    Xdat1, Xdat2, Xdat3 = np.array_split(self.XData, 3)\n",
        "    zoomedXData = self.zoom(Xdat1)\n",
        "    XData = np.append(self.XData, zoomedXData, axis = 0)\n",
        "    flippedXData = self.flip(Xdat2)\n",
        "    XData = np.append(XData, flippedXData, axis = 0)\n",
        "\n",
        "    blurredXData = self.blur(Xdat3)\n",
        "    XData = np.append(XData, blurredXData, axis = 0)\n",
        "\n",
        "    # XData = np.stack((self.XData, zoomedXData, flippedXData, blurredXData),axis = 0)\n",
        "    YData = np.tile(self.YData,2)\n",
        "\n",
        "    return XData, YData\n",
        "\n",
        "\n",
        "X_train, y_train = Augmenter(X_train, y_train).run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNx34d19TQtB"
      },
      "source": [
        "## Baseline - Perceptron\n",
        "For the baseline model, we simply flatten the image data and utlize Perceptron as our classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGEXe3TITN3_"
      },
      "source": [
        "def flatten_generator(images):\n",
        "  for image in images:\n",
        "    yield image.flatten()\n",
        "\n",
        "def flatten(images):\n",
        "  '''TODO: Flatten the Image Data\n",
        "  Input: (128, 128, 3)\n",
        "  Output: (49152,)\n",
        "  '''\n",
        "  return list(flatten_generator(images))\n",
        "\n",
        "X_train_flatten = flatten(X_train)\n",
        "X_val_flatten = flatten(X_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COObNGefbI4t"
      },
      "source": [
        "from sklearn.linear_model import Perceptron\n",
        "### TODO: Use Perceptron to fit the training data ###\n",
        "model = Perceptron()\n",
        "model.fit(X_train_flatten, y_train)\n",
        "print(model.score(X_train_flatten, y_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaSQULVcX2Wo"
      },
      "source": [
        "### Evaluate the performance on the test images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wMpAteOWsBj"
      },
      "source": [
        "!wget \"https://drive.google.com/uc?export=download&id=16CCS6DiAzFwCT1165ogvKVf9JwajOAHm\" -O test_images.p\n",
        "test_images = pickle.load(open(\"test_images.p\", \"rb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7Y_fl1faez4"
      },
      "source": [
        "You will need to preprocess these test images and predict the label for each images (1: good, 2: bad) \n",
        "\n",
        "and save your predictions in a list [0, 1, 1, ...]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dkk9QAeUa6Ay"
      },
      "source": [
        "###TODO: Use your perceptron model to predict on the test images###\n",
        "prediction = []\n",
        "prediction = model.predict(X_val_flatten)\n",
        "#print(prediction)\n",
        "\n",
        "# Do not change the following line, you need to submit perceptron.p to gradescope\n",
        "# If you are using colab, it will show up on the left side, remember to download it\n",
        "pickle.dump(prediction, open(\"perceptron.p\", \"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IM2bCnPDb3Au"
      },
      "source": [
        "## Build Your Own CNN Model\n",
        "To improve the performance of our mask detector, we plan to build a CNN model using keras. A suggested architecture is shown as follows, but feel free to modify it by adding or eliminating layers. The autograder is based on your final accuracy.\n",
        "\n",
        "\n",
        "* Convolution with 32 filters with kernel size 7x7 followed by ReLU activation \n",
        "function, input shape (128, 128, 3);\n",
        "* Max Pool with filter size/pool size = 7 and stride = 4;\n",
        "* Convolution with 16 filters with kernel size 5x5 followed by ReLU activation function;\n",
        "* Max Pool with filter size/pool size = 7 and stride = 4;\n",
        "* Flatten layer to transform 3D layers to a single tensor/vector;\n",
        "* Fully Connected with 64 neurons and ReLU activation function\n",
        "* Fully Connected with 2 neurons and softmax activation function\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2rOyGZzb2J0"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, Conv1D, MaxPooling1D\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZBSwED5ad4o"
      },
      "source": [
        "model = Sequential()\n",
        "num_classes = 2\n",
        "\n",
        "# # TODO: Conv1\n",
        "model.add(Conv2D(32, 7, activation='relu', input_shape=(128, 128, 3)))\n",
        "model.add(MaxPooling2D(pool_size=(7, 7), strides = 4))\n",
        "\n",
        "# # TODO: Conv2\n",
        "model.add(Conv2D(16, 5, activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(7, 7), strides = 4))\n",
        "\n",
        "# # TODO: Flatten the layer\n",
        "model.add(Flatten())\n",
        "\n",
        "# # TODO: Add the intermediate fully connected layers (Dense in keras)\n",
        "model.add(Dense(64, activation='relu'))\n",
        "\n",
        "# # TODO: Add the final fully connected layer with the softmax activation function\n",
        "model.add(Dense(2, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wu3wDDr1bBGy"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4WY4xVgbzs8"
      },
      "source": [
        "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy']) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YrE_1LBd04q"
      },
      "source": [
        "### Ready For Training\n",
        "Before training, we need to first make sure the training data is an array with the correct size of our model's input. We also need to convert our labels to one hot type."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9cXgmExdQ8U"
      },
      "source": [
        "def convert2onehot(labels):\n",
        "  result = []\n",
        "  for label in labels:\n",
        "    if label == 1:\n",
        "      result.append([1, 0])\n",
        "    else:\n",
        "      result.append([0, 1])\n",
        "  return np.array(result)\n",
        "\n",
        "X_train_array = np.array(X_train)\n",
        "X_val_array = np.array(X_val)\n",
        "y_train_onehot = convert2onehot(y_train)\n",
        "y_val_onehot = convert2onehot(y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_A-iUn0ac-pm"
      },
      "source": [
        "model.fit(X_train_array, y_train_onehot, batch_size=16, epochs=10, validation_data=(X_val_array, y_val_onehot))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6YP10CUgG-u"
      },
      "source": [
        "### Evaluate on test images\n",
        "Again, evaluate the performance of your model on the test images and save your predictions into a list `[0, 1, 0, 0, ...]`.\n",
        "\n",
        "Note that you need to convert the one-hot prediction to the original labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxDxUcrQDSHA"
      },
      "source": [
        "###TODO: Use the CNN model to predict on the test images###\n",
        "prediction = []\n",
        "\n",
        "predictions = model.predict(X_val_array)\n",
        "for p in predictions:\n",
        "  if p[0] >= p[1]:\n",
        "    prediction.append(1)\n",
        "  else:\n",
        "    prediction.append(0)\n",
        "\n",
        "print(prediction)\n",
        "\n",
        "# Do not change the following line, you need to submit cnn.p to gradescope\n",
        "# If you are using colab, it will show up on the left side, remember to download it\n",
        "pickle.dump(prediction, open(\"cnn.p\", \"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZPdHb-ehejn"
      },
      "source": [
        "## Advanced CNN Model using ImageNet\n",
        "To further enhance the ability of R2D2, we decided to use some well designed network architectures, aka, Imagenet. There are bunch of imagenets embedded in keras and you could follow the examples from [official document](https://keras.io/api/applications/) and fine tune these model on our mask detection task.\n",
        "\n",
        "Once you construct a excellent model, remember to save the model configuration and weight, as well as evaluate on the test images and upload to the leaderboard to compete with your classmates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFpkgJCILqpd"
      },
      "source": [
        "from keras.applications import InceptionResNetV2\n",
        "from tensorflow.python.keras.layers import Flatten, Dense, Dropout\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "net = InceptionResNetV2(include_top=False,\n",
        "                        weights='imagenet',\n",
        "                        input_tensor=None,\n",
        "                        input_shape=(128, 128, 3),\n",
        "                        pooling='max')\n",
        "num_classes = 2\n",
        "\n",
        "x = net.output\n",
        "x = Flatten()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "output_layer = Dense(num_classes, activation='softmax', name='softmax')(x)\n",
        "net_final = Model(inputs=net.input, outputs=output_layer)\n",
        "for layer in net_final.layers[:2]:\n",
        "  layer.trainable = False\n",
        "for layer in net_final.layers[2:]:\n",
        "  layer.trainable = True\n",
        "net_final.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy']) \n",
        "#net_final.compile(optimizer=Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy']) \n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    fill_mode='nearest',\n",
        "    vertical_flip=True,\n",
        "    horizontal_flip=True)\n",
        "earlystopping_callback = EarlyStopping(monitor='val_accuracy', mode='max', patience=20, restore_best_weights=True)\n",
        "bestweights_callback= ModelCheckpoint(filepath='/bestweightscheckpoint', monitor='val_accuracy', mode='max', save_best_only=True, save_weights_only=True)\n",
        "\n",
        "net_final.fit(datagen.flow(X_train_array, y_train_onehot, batch_size=32, shuffle=True), steps_per_epoch=len(X_train_array) / 32, epochs=20, validation_data=(X_val_array, y_val_onehot), callbacks=[earlystopping_callback, bestweights_callback])\n",
        "net_final.load_weights('/bestweightscheckpoint')\n",
        "\n",
        "\n",
        "########################Alternative model###############################\n",
        "# net = InceptionResNetV2(include_top=False,\n",
        "#                         weights='imagenet',\n",
        "#                         input_tensor=None,\n",
        "#                         input_shape=(128, 128, 3),\n",
        "#                         pooling='max') #'max' vs 'avg'\n",
        "# num_classes = 2\n",
        "\n",
        "# x = net.output\n",
        "# x = Flatten()(x)\n",
        "# x = Dropout(0.5)(x) #Put back in for 97.5\n",
        "# output_layer = Dense(num_classes, activation='softmax', name='softmax')(x)\n",
        "# net_final = Model(inputs=net.input, outputs=output_layer)\n",
        "# for layer in net_final.layers[:2]:\n",
        "#   layer.trainable = False\n",
        "# for layer in net_final.layers[2:]:\n",
        "#   layer.trainable = True\n",
        "# net_final.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy']) \n",
        "# #net_final.compile(optimizer=Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy']) \n",
        "\n",
        "# datagen = ImageDataGenerator(\n",
        "#     rotation_range=40,\n",
        "#     width_shift_range=0.2,\n",
        "#     height_shift_range=0.2,\n",
        "#     shear_range=0.2,\n",
        "#     zoom_range=0.2,\n",
        "#     fill_mode='nearest',\n",
        "#     vertical_flip=True,\n",
        "#     horizontal_flip=True)\n",
        "# net_final.fit(datagen.flow(X_train_array, y_train_onehot, batch_size=32, shuffle=True), steps_per_epoch=len(X_train_array) / 32, epochs=20, validation_data=(X_val_array, y_val_onehot))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gI8uMMwQ6--E"
      },
      "source": [
        "prediction = []\n",
        "processed_test_images = []\n",
        "for image in test_images:\n",
        "  processed_test_images.append(preprocess(image))\n",
        "X_test_array = np.array(processed_test_images)\n",
        "X_test_array_flattened = flatten(X_test_array)\n",
        "\n",
        "prediction_one_hot = net_final.predict(X_test_array)\n",
        "\n",
        "def one_hot_to_labels(prediction):\n",
        "  def yielder(prediction):\n",
        "    for elem in prediction_one_hot:\n",
        "      elem = np.rint(elem).astype(int)\n",
        "      if all(elem == np.array([1,0])):\n",
        "        yield 1\n",
        "      elif all(elem == np.array([0,1])):\n",
        "        yield 0\n",
        "  return list(yielder(prediction))\n",
        "\n",
        "prediction = one_hot_to_labels(prediction_one_hot)\n",
        "print(len(prediction))\n",
        "print(prediction)\n",
        "\n",
        "# Do not change the following line, you need to submit best.p to gradescope\n",
        "# If you are using colab, it will show up on the left side, remember to download it\n",
        "pickle.dump(prediction, open(\"best.p\", \"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}